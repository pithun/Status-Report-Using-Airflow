{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Airflow?\n",
    "From my understanding, it's a way of scheduling tasks in a structured way and fosters automation. I'm reading a book and I'm starting with the part 1 which focuses on the basics of Airflow, explaining what Airflow is and outlining its\n",
    "basic concepts. \n",
    "\n",
    "I like the description of Airflow as a spider in a web as it sits within different systems and controls processes. We describe tasks in form of sequences (which form a pipeline) and it's easier to represent these tasks in a directed graphical order. Hence, Airflow uses a concept of DAG which is Directed Acyclic Graph. The acyclic property means no two taks have circular dependency (where a depends on b and b on a).\n",
    "\n",
    "Airflow executes taks in a loop, say there's three tasks \n",
    "a -> b -> c\n",
    "\n",
    "On the first iter, there's no task a is dependent on, so it's added to queue and is executed but on the first iter, b depends on a and a isn't run and c depends on b which isn't run, so they can't be added to the queue."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How exactly do we do this execution of ordered tasks?\n",
    "It seems each DAG (task) is a single python script?? No, a DAG is the entire pipeline of tasks. It seems somehow, we define the structure of our Pipline in the DAG (script) and it's executed in that order. Next up is on scheduling and executing pipelines. \n",
    "\n",
    "### Scheduling and DAG execution in Airflow\n",
    "At a high level, Airflow is organized into three main components; \n",
    "- The Airflow scheduler—Parses DAGs, checks their schedule interval, and (if the\n",
    "DAGs’ schedule has passed) starts scheduling the DAGs’ tasks for execution by\n",
    "passing them to the Airflow workers.\n",
    "- The Airflow workers—Pick up tasks that are scheduled for execution and execute\n",
    "them. As such, the workers are responsible for actually “doing the work.”\n",
    "- The Airflow webserver—Visualizes the DAGs parsed by the scheduler and provides\n",
    "the main interface for users to monitor DAG runs and their results\n",
    "\n",
    "There's some talk about being able to run past schedules. Thus, getting historical data because it seems that Airflow is able to run on specific data (an incremental kind of thing) for each schedule and not on an entire dataset??\n",
    "Refer to your implemented DAG on status report and read comments for more.\n",
    "\n",
    "### Monitoring and  Handling Failures\n",
    "This can be done on Airflows webserver. \n",
    "\n",
    "Next I proceed to implementing my first DAG which is based on a scenario where a guy want to download images about future rocket launches into his pc and get notified. He wanted to fetch the info about all rocket launches into his computer, next extract the image url and go to the image site and download into his computer. Finally, he wanted to get notified about it. We can represent this in DAG form as\n",
    "\n",
    "Download launches -> Get Images -> notify\n",
    "\n",
    "Actually to see the logs of a process, you have to click in the graph space and just above in that graph space, you'll see the logs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Airflow\n",
    "\n",
    "With Python on Anaconda\n",
    "\n",
    "After creating a virtual env -> `conda create --name Airflow_active python==3.11`\n",
    "We install apache-ariflow -> there was an error so I had to install one microsoft visual app which didn't  solve it. Modifying the package downloaded from the ms visual took about 2gb. Then we use `pip install apache-airflow`\n",
    "\n",
    "\n",
    "Then I ran the below in the order\n",
    "\n",
    "airflow db init\n",
    "\n",
    "airflow users create --username admin --password admin --firstname Chigozie --lastname Udoh --role Admin --email udohchigozie2017@gmail.com\n",
    "\n",
    "I used this to copy in anaconda `copy DAGs\\. C:\\Users\\User\\airflow\\dags`\n",
    "\n",
    "next was to run the web server but I was getting the below error and had to use kali-linux shell via windows subsystem for linux (WSL)\n",
    "File \"C:\\Users\\User\\anaconda3\\envs\\Airflow_active\\Lib\\site-packages\\daemon\\daemon.py\", line 13, in <module>\n",
    "    import pwd\n",
    "ModuleNotFoundError: No module named 'pwd'\n",
    "\n",
    "\n",
    "### Working Run of airflow with python\n",
    " There was a problem seems some pwd module is not available for windows https://github.com/apache/airflow/discussions/24323 so I had to use wsl kali linux. Refer here \n",
    " https://medium.com/@tristian_56632/installing-apache-airflow-on-windows-usng-wsl-771e803762c9\n",
    "\n",
    "Steps -> Kali airflow db path \"/home/pithun/airflow\"\n",
    "1. I already had kali linux installed with the WSL2 setup\n",
    "2. run `kali-linux`\n",
    "3. run `sudo apt update` to update the package lists to ensure you have the latest information about available packages\n",
    "4. run `sudo apt install -y python3.11 python3.11-dev python3-pip python3-venv` to install python 3.11\n",
    "5. Create a virtual environment `python3 -m venv Airflow_Thunder`\n",
    "6. Activate your virtual environment `source Airflow_Thunder/bin/activate`\n",
    "7. Initialize airflow db file `airflow db init`\n",
    "8. Input admin details `airflow users create --username admin --password admin --firstname Chigozie --lastname Udoh --role Admin --email udohchigozie2017@gmail.com`\n",
    "9. Create a dag directory in the airflow dir cd `/home/pithun/airflow` -> `mkdir dags`\n",
    "10. Copy DAG files from your local to airflow dir. In the Kali-Linux shell, my windows path was in `mnt/Users/User`. Then I copied using cp \"path/DAGs/.\" \"/home/pithun/airflow\".\n",
    "11. Install the necessary modules required by your DAG e.g `pip install seaborn`\n",
    "12. Start the webserver with an optionally defined port `airflow webserver -p 8877`\n",
    "13. In a seperate kali window start the airflow scheduler `airflow scheduler`\n",
    "\n",
    "\n",
    " \n",
    "\n",
    " \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Running Docker Container for airflow\n",
    "Finally installed airflow. I made a docker image for it with the below command. I can copy DAGs into the container and run them. I'll figure it out somehow.\n",
    "docker run -ti -p 8080:8080 -v \"C:/Users/User/Documents/Important Docs/Data Internships/Data Engineering/Everlytics/from Bootcamp to Internship/Learning/Internship Learning/AirFlow/Rocket_Launch_DAG.py:/opt/airflow/dags/Rocket_Launch_DAG.py\" --entrypoint=/bin/bash --name airflow apache/airflow:2.0.0-python3.8 -c '( airflow db init && airflow users create --username admin --password admin --firstname Pithun --lastname Admin --role Admin --email udohchigozie2017@gmail.com ); airflow webserver & airflow scheduler'\n",
    "\n",
    "\n",
    "\n",
    "### RUNNING THE CONTAINER AT A LATER TIME\n",
    "I think because there's like three different servers to host actually, it's usually better to have three containers or say 3 nodes. However, running this command starts airflow. I'll try running it without specifying all the details the next time.\n",
    "\n",
    "1. I used dockerfile to rebuild the image with the relevant python directories installed via a requirements.txt file\n",
    "2. I downloaded the configurations into a yml file with new image to run it. I just need to run `docker-compose up` to start it. To stop the container, pause it in docker. CAUTION! `docker-compose down` deletes the container.\n",
    "3. After running the DAG and all I used `docker exec -it airflow-airflow-1 /bin/bash` to get into the container from my powershell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My first DAG \n",
    "I was able to run my first DAG and saved the output to my PC in `/mnt/c/Users/User/Viz`    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 3\n",
    "Scheduling in Airflow\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can schedule airflow to run on weekly, hourly basis and stuff. There's a cron based management or representation for setting airflow time. We basically use 5 numbers os asterik we could say 0**** the first position means minutes, hour, day of month, month, day of week. Asterik means we don't care about that position.\n",
    "\n",
    "Since say the minutes in an hour is 60 in (0-59) 0 represents 60. I can define a daily schedule as *0*** in this case I'm specifying 0 i.e 24 hour as the interval which means the day will run by midnight everyday. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
